{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "dacon_2th_CV_dirtymnist_5th_place_Ensembled.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7MCFSp1IWxN"
      },
      "source": [
        "# __Code__\n",
        "\n",
        "베이스라인 코드에서 크게 수정하지는 않았습니다.  \n",
        "EfficientNet-B7 모델 2개로 앙상블을 수행하였고,  \n",
        "**실제 학습을 수행하였을 때는 세션 여러개를 띄워 각 fold를 할당하여 학습하였습니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3R8R6b5l0_3"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onRt9Uj9cVMG"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0rPHhGmJm6o"
      },
      "source": [
        "# __Data__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVyd9cclchJu"
      },
      "source": [
        "from google.colab import output\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Dacon_Emnist2/data_2.zip\" \"data_2.zip\"\n",
        "!unzip \"data_2.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV4eAWLndJq6"
      },
      "source": [
        "!mkdir \"./dirty_mnist\"\n",
        "!unzip \"dirty_mnist_2nd.zip\" -d \"./dirty_mnist/\"\n",
        "!mkdir \"./test_dirty_mnist\"\n",
        "!unzip \"test_dirty_mnist_2nd.zip\" -d \"./test_dirty_mnist/\"\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PinxcACJJxrk"
      },
      "source": [
        "# __Library Import & Download Efficentnet_pytorch__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfOe9SS-UYEx"
      },
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuOUS8OweElM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import imutils\n",
        "import zipfile\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iT1VM2zjNtS"
      },
      "source": [
        "dirty_mnist_answer = pd.read_csv(\"dirty_mnist_2nd_answer.csv\")\n",
        "dirty_mnist_answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-nbpapbKkAh"
      },
      "source": [
        "# __Data Preprocessing__\n",
        "\n",
        "RandomFlip을 비롯한 다양한 Augmentation을 실험하였지만  \n",
        "오히려 성능이 하락하였기 때문에  \n",
        "간단하게 RandomRoation만 적용하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k8vZN2Tlamy"
      },
      "source": [
        "namelist = os.listdir('./dirty_mnist/')\n",
        "\n",
        "# numpy를 tensor로 변환하는 ToTensor 정의\n",
        "class ToTensor(object):\n",
        "    \"\"\"numpy array를 tensor(torch)로 변환합니다.\"\"\"\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample['image'], sample['label']\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return {'image': torch.FloatTensor(image),\n",
        "                'label': torch.FloatTensor(label)}\n",
        "# to_tensor 선언\n",
        "\n",
        "to_tensor = T.Compose([\n",
        "                      ToTensor()\n",
        "])\n",
        "\n",
        "# Augmentation은 horizontal, vertical flip도 넣고 실험해봤지만\n",
        "# 성능이 오히려 떨어짐 -> RandomRotation만 사용\n",
        "augmentations = T.Compose([\n",
        "                           T.ToPILImage(),\n",
        "                           T.RandomRotation(40),\n",
        "                           T.ToTensor()\n",
        "                           \n",
        "])\n",
        "\n",
        "class DatasetMNIST(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "                 dir_path,\n",
        "                 meta_df,\n",
        "                 transforms=to_tensor,#미리 선언한 to_tensor를 transforms로 받음\n",
        "                 augmentations=None):\n",
        "        \n",
        "        self.dir_path = dir_path # 데이터의 이미지가 저장된 디렉터리 경로\n",
        "        self.meta_df = meta_df # 데이터의 인덱스와 정답지가 들어있는 DataFrame\n",
        "\n",
        "        self.transforms = transforms# Transform\n",
        "        self.augmentations = augmentations # Augmentation\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.meta_df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # 폴더 경로 + 이미지 이름 + .png => 파일의 경로\n",
        "        # 참고) \"12\".zfill(5) => 000012\n",
        "        #       \"146\".zfill(5) => 000145\n",
        "        # cv2.IMREAD_GRAYSCALE : png파일을 채널이 1개인 GRAYSCALE로 읽음\n",
        "        image = cv2.imread(self.dir_path +\\\n",
        "                           str(self.meta_df.iloc[index,0]).zfill(5) + '.png',\n",
        "                           cv2.IMREAD_GRAYSCALE)\n",
        "        # 0 ~ 255의 값을 갖고 크기가 (256,256)인 numpy array를\n",
        "        # 0 ~ 1 사이의 실수를 갖고 크기가 (256,256,1)인 numpy array로 변환\n",
        "        image = (image/255).astype('float32')[..., np.newaxis]\n",
        "\n",
        "        # 정답 numpy array생성(존재하면 1 없으면 0)\n",
        "        label = self.meta_df.iloc[index, 1:].values.astype('float')\n",
        "        sample = {'image': image, 'label': label}\n",
        "\n",
        "        # transform 적용\n",
        "        # numpy to tensor\n",
        "        if self.transforms:\n",
        "            sample = self.transforms(sample)\n",
        "        if self.augmentations:\n",
        "            sample['image'] = self.augmentations(sample['image'])\n",
        "        # sample 반환\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaMlZNZsK3db"
      },
      "source": [
        "# __Model__\n",
        "\n",
        "두명의 팀원 모두 EfficientNet-B7으로 진행.  \n",
        "두 모델은 출력 이전 층에서 dropout의 적용 여부 및 activation 함수의 종류만 다르고  \n",
        "나머지는 동일합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giLYEJtTld0z"
      },
      "source": [
        "################# H.J.S Model #################\n",
        "\n",
        "class MultiLabelEfficientnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiLabelEfficientnet, self).__init__()\n",
        "        self.conv2d = nn.Conv2d(1, 3, 3, stride=1)\n",
        "        self.efficientnet = EfficientNet.from_pretrained('efficientnet-b7')\n",
        "        self.drop = nn.Dropout(p=0.2)\n",
        "        self.FC = nn.Linear(1000, 26)\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = F.silu(self.conv2d(x))\n",
        "\n",
        "        # efficientnet b7\n",
        "        # 원래는 efficientnet과 동일하게 swish를 넣으려고 했으나\n",
        "        # 별 차이 없는 듯 해서 그냥 silu 적용\n",
        "        x = F.silu(self.efficientnet(x))\n",
        "\n",
        "        # 마지막 출력에 nn.Linear를 추가\n",
        "        # multilabel을 예측해야 하기 때문에\n",
        "        # softmax가 아닌 sigmoid를 적용\n",
        "        # dropout 추가\n",
        "        x = self.drop(x)\n",
        "        x = torch.sigmoid(self.FC(x))\n",
        "        return x\n",
        "# 모델 선언\n",
        "model = MultiLabelEfficientnet()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5j2Ko_zAFaq"
      },
      "source": [
        "################# 잉돌 Model #################\n",
        "\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "# A memory-efficient implementation of Swish function\n",
        "class SwishImplementation(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, i):\n",
        "        result = i * torch.sigmoid(i)\n",
        "        ctx.save_for_backward(i)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        i = ctx.saved_tensors[0]\n",
        "        sigmoid_i = torch.sigmoid(i)\n",
        "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
        "\n",
        "class MemoryEfficientSwish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return SwishImplementation.apply(x)\n",
        "        \n",
        "class MultiLabelEfficientnet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiLabelEfficientnet2, self).__init__()\n",
        "        self.conv2d = nn.Conv2d(1, 3, 3, stride=1)\n",
        "        self._swish = MemoryEfficientSwish()\n",
        "        self.efficientnet = EfficientNet.from_pretrained('efficientnet-b7')\n",
        "        self.fc = nn.Linear(1000, 26)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 3개의 채널(color)을 갖기 때문에\n",
        "        # 입력 전에 conv2d를 한 층 추가\n",
        "        # swish 사용\n",
        "        x = self._swish(self.conv2d(x))\n",
        "        # effnet 추가\n",
        "        x = self._swish(self.efficientnet(x))\n",
        "        # 마지막 출력에 nn.Linear를 추가\n",
        "        # multilabel을 예측해야 하기 때문에\n",
        "        # softmax가 아닌 sigmoid 적용(0~1)\n",
        "        x = torch.sigmoid(self.fc(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# 모델 선언\n",
        "model2 = MultiLabelEfficientnet2()\n",
        "# model2.eval()\n",
        "model2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_3ZhGWdL6MP"
      },
      "source": [
        "# __Train__\n",
        "\n",
        "실제 학습은 두 명이 각자 수행하였고,  \n",
        "베이스라인 코드와 마찬가지로 5 fold로 진행하였습니다.  \n",
        "아래 코드와는 다르게 실제 훈련 시에는 세션 여러개를 띄워  \n",
        "각 fold를 할당하여 진행하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw4Pdq03lpUx"
      },
      "source": [
        "##### MultiLabelEfficientnet Train #####\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# dirty_mnist_answer에서 train_idx와 val_idx를 생성\n",
        "best_models = [] # 폴드별로 가장 validation acc가 높은 모델 저장\n",
        "for fold_index, (trn_idx, val_idx) in enumerate(kfold.split(dirty_mnist_answer),1):\n",
        "    \n",
        "    #########################################\n",
        "\n",
        "    # 다섯 폴드를 코랩에서 그대로 다 돌리는 것은 불가능해서\n",
        "    # 사본 여러개 띄우고 폴드 할당해서 훈련 + 저장\n",
        "    # evaluate은 저장한 폴드별 best model을 불러와서 사용\n",
        "\n",
        "    # 1\n",
        "    # if fold_index > 1:\n",
        "    #   break\n",
        "    \n",
        "    # 2\n",
        "    # if fold_index == 1:\n",
        "    #   continue\n",
        "    # elif fold_index > 2:\n",
        "    #   break\n",
        "\n",
        "    # 3\n",
        "    # if fold_index < 3:\n",
        "    #   continue\n",
        "    # elif fold_index > 3:\n",
        "    #   break\n",
        "\n",
        "    #4\n",
        "    # if fold_index < 4:\n",
        "    #   continue\n",
        "    # elif fold_index > 4:\n",
        "    #   break\n",
        "\n",
        "    # 5\n",
        "    # if fold_index != 5:\n",
        "    #   break\n",
        "\n",
        "    #########################################\n",
        "    \n",
        "    print(f'[fold: {fold_index}]')\n",
        "    \n",
        "    # cuda cache 초기화\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    #train fold, validation fold 분할\n",
        "    train_answer = dirty_mnist_answer.iloc[trn_idx]\n",
        "    test_answer  = dirty_mnist_answer.iloc[val_idx]\n",
        "\n",
        "    #Dataset 정의\n",
        "    train_dataset = DatasetMNIST(\"dirty_mnist/\", train_answer, augmentations=augmentations)\n",
        "    valid_dataset = DatasetMNIST(\"dirty_mnist/\", test_answer)\n",
        "\n",
        "    #DataLoader 정의\n",
        "    train_data_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size = 16,\n",
        "        shuffle = True,\n",
        "        num_workers = 3\n",
        "    )\n",
        "    valid_data_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size = 8,\n",
        "        shuffle = False,\n",
        "        num_workers = 3\n",
        "    )\n",
        "\n",
        "    # 모델 선언\n",
        "    model = MultiLabelEfficientnet()\n",
        "    model.to(device)# gpu에 모델 할당\n",
        "\n",
        "    # 훈련 옵션 설정\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                lr = 0.001)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size = 5,\n",
        "                                                gamma = 0.9)\n",
        "    criterion = torch.nn.BCELoss()\n",
        "    # 훈련 시작\n",
        "    valid_acc_max = 0\n",
        "    valid_loss_min = float(\"inf\")\n",
        "    for epoch in range(30):\n",
        "        # 1개 epoch 훈련\n",
        "        train_acc_list = []\n",
        "        train_loss_list = []\n",
        "        with tqdm(train_data_loader,#train_data_loader를 iterative하게 반환\n",
        "                total=train_data_loader.__len__(), # train_data_loader의 크기\n",
        "                unit=\"batch\") as train_bar:# 한번 반환하는 sample의 단위는 \"batch\"\n",
        "            for sample in train_bar:\n",
        "                train_bar.set_description(f\"Train Epoch {epoch}\")\n",
        "               \n",
        "                optimizer.zero_grad()\n",
        "                images, labels = sample['image'], sample['label']\n",
        "                # tensor를 gpu에 올리기 \n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # 모델의 dropoupt, batchnormalization를 train 모드로 설정\n",
        "                model.train()\n",
        "                # .forward()에서 중간 노드의 gradient를 계산\n",
        "                with torch.set_grad_enabled(True):\n",
        "                    # 모델 예측\n",
        "                    probs  = model(images)\n",
        "                    # loss 계산\n",
        "                    loss = criterion(probs, labels)\n",
        "                    # 중간 노드의 gradient로\n",
        "                    # backpropagation을 적용하여\n",
        "                    # gradient 계산\n",
        "                    loss.backward()\n",
        "                    # weight 갱신\n",
        "                    optimizer.step()\n",
        "\n",
        "                    # train accuracy 계산\n",
        "                    probs  = probs.cpu().detach().numpy()\n",
        "                    labels = labels.cpu().detach().numpy()\n",
        "                    preds = probs > 0.5\n",
        "                    batch_acc = (labels == preds).mean()    \n",
        "                    train_acc_list.append(batch_acc)\n",
        "                    train_acc = np.mean(train_acc_list)\n",
        "                    train_loss_list.append(loss.item())\n",
        "                    train_loss = np.mean(train_loss_list)\n",
        "                # 현재 progress bar에 현재 미니배치의 loss 결과 출력\n",
        "                train_bar.set_postfix(train_loss= train_loss,\n",
        "                                      train_acc = train_acc)\n",
        "                \n",
        "\n",
        "        # 1개 epoch학습 후 Validation 점수 계산\n",
        "        valid_acc_list = []\n",
        "        valid_loss_list = []\n",
        "        with tqdm(valid_data_loader,\n",
        "                total=valid_data_loader.__len__(),\n",
        "                unit=\"batch\") as valid_bar:\n",
        "            for sample in valid_bar:\n",
        "                valid_bar.set_description(f\"Valid Epoch {epoch}\")\n",
        "                optimizer.zero_grad()\n",
        "                images, labels = sample['image'], sample['label']\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # 모델의 dropoupt, batchnormalization를 eval모드로 설정\n",
        "                model.eval()\n",
        "                # .forward()에서 중간 노드의 gradient를 계산\n",
        "                with torch.no_grad():\n",
        "                    # validation loss만을 계산\n",
        "                    probs  = model(images)\n",
        "                    valid_loss = criterion(probs, labels)\n",
        "\n",
        "                    # train accuracy 계산\n",
        "                    probs  = probs.cpu().detach().numpy()\n",
        "                    labels = labels.cpu().detach().numpy()\n",
        "                    preds = probs > 0.5\n",
        "                    batch_acc = (labels == preds).mean()\n",
        "                    valid_acc_list.append(batch_acc)\n",
        "                    valid_loss_list.append(valid_loss.item())\n",
        "                valid_acc = np.mean(valid_acc_list)\n",
        "                valid_loss = np.mean(valid_loss_list)\n",
        "                valid_bar.set_postfix(valid_loss = valid_loss,\n",
        "                                      valid_acc = valid_acc)\n",
        "            \n",
        "        # Learning rate 조절\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # 모델 저장\n",
        "        if valid_loss_min > valid_loss:\n",
        "            valid_loss_min = valid_loss\n",
        "            best_model = model\n",
        "            \n",
        "            \n",
        "\n",
        "    # 폴드별로 가장 좋은 모델 저장\n",
        "    MODEL = \"efficientnetb7\"\n",
        "    # 모델을 저장할 구글 드라이브 경로\n",
        "    path = \"/content/drive/MyDrive/Dacon_Emnist2/\"\n",
        "    torch.save(best_model, f'{path}{MODEL}_{fold_index}_{valid_loss:2.4f}.pth')\n",
        "    best_models.append(best_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KcjJSbjAbcJ"
      },
      "source": [
        "##### MultiLabelEfficientnet2 Train #####\n",
        "\n",
        "# cross validation을 적용하기 위해 KFold 생성\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# dirty_mnist_answer에서 train_idx와 val_idx를 생성\n",
        "best_models2 = [] # 폴드별로 가장 validation acc가 높은 모델 저장\n",
        "for fold_index, (trn_idx, val_idx) in enumerate(kfold.split(dirty_mnist_answer),1):\n",
        "\n",
        "    # if fold_index > 1:\n",
        "    #   break\n",
        "\n",
        "    print(f'[fold: {fold_index}]')\n",
        "    # cuda cache 초기화\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    #train fold, validation fold 분할\n",
        "    train_answer = dirty_mnist_answer.iloc[trn_idx]\n",
        "    valid_answer  = dirty_mnist_answer.iloc[val_idx]\n",
        "\n",
        "    #Dataset 정의\n",
        "    train_dataset = DatasetMNIST(\"dirty_mnist/\", train_answer, augmentations=augmentations) # use augmentations param at trainset\n",
        "    valid_dataset = DatasetMNIST(\"dirty_mnist/\", valid_answer)\n",
        "\n",
        "    #DataLoader 정의\n",
        "    train_data_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size = 16,\n",
        "        shuffle = False,\n",
        "        num_workers = 3\n",
        "    )\n",
        "    valid_data_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size = 8,\n",
        "        shuffle = False,\n",
        "        num_workers = 3\n",
        "    )\n",
        "\n",
        "    model2 = MultiLabelEfficientnet2() #모델 선언\n",
        "    model2.to(device)# gpu에 모델 할당\n",
        "\n",
        "    # 훈련 옵션 설정\n",
        "    optimizer = torch.optim.Adam(model2.parameters(),\n",
        "                                lr = 0.001)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size = 5, #for each 5 epochs\n",
        "                                                gamma = 0.85) #current_lr*0.85\n",
        "    #binary cross entropy\n",
        "    criterion = torch.nn.BCELoss()\n",
        "\n",
        "    # 훈련 시작\n",
        "    valid_acc_max = 0\n",
        "    for epoch in range(30): #30 epoch\n",
        "        # 1개 epoch 훈련\n",
        "        train_acc_list = []\n",
        "        train_loss_list = []\n",
        "        with tqdm(train_data_loader,#train_data_loader를 iterative하게 반환\n",
        "                  total=train_data_loader.__len__(), # train_data_loader의 크기\n",
        "                  unit=\"batch\"\n",
        "                  ) as train_bar:# 한번 반환하는 sample의 단위는 \"batch\"\n",
        "            for sample in train_bar:\n",
        "                train_bar.set_description(f\"Train Epoch {epoch}\")\n",
        "                # 갱신할 변수들에 대한 모든 변화도를 0으로 초기화\n",
        "                # 참고)https://tutorials.pytorch.kr/beginner/pytorch_with_examples.html\n",
        "                optimizer.zero_grad()\n",
        "                images, labels = sample['image'], sample['label']\n",
        "                # tensor를 gpu에 올리기 \n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # 모델의 dropoupt, batchnormalization를 train 모드로 설정\n",
        "                model2.train()\n",
        "                # .forward()에서 중간 노드의 gradient를 계산\n",
        "                with torch.set_grad_enabled(True):\n",
        "                    # 모델 예측\n",
        "                    probs  = model2(images)\n",
        "                    # loss 계산\n",
        "                    loss = criterion(probs, labels)\n",
        "                    # 중간 노드의 gradient로\n",
        "                    # backpropagation을 적용하여\n",
        "                    # gradient 계산\n",
        "                    loss.backward()\n",
        "                    # weight 갱신\n",
        "                    optimizer.step()\n",
        "                    # train accuracy 및 loss 계산\n",
        "                    probs  = probs.cpu().detach().numpy()\n",
        "                    labels = labels.cpu().detach().numpy()\n",
        "                    preds = probs > 0.5\n",
        "                    batch_acc = (labels == preds).mean()    \n",
        "                    train_acc_list.append(batch_acc)\n",
        "                    train_acc = np.mean(train_acc_list)\n",
        "                    train_loss_list.append(loss.item())\n",
        "                    train_loss = np.mean(train_loss_list)\n",
        "                # 현재 progress bar에 현재 미니배치의 loss 결과 출력\n",
        "                train_bar.set_postfix(train_loss= train_loss,\n",
        "                                      train_acc = train_acc)\n",
        "                \n",
        "\n",
        "        # 1개 epoch학습 후 Validation 점수 계산\n",
        "        valid_acc_list = []\n",
        "        valid_loss_list = []\n",
        "        with tqdm(valid_data_loader,\n",
        "                total=valid_data_loader.__len__(),\n",
        "                unit=\"batch\") as valid_bar:\n",
        "            for sample in valid_bar:\n",
        "                valid_bar.set_description(f\"Valid Epoch {epoch}\")\n",
        "                optimizer.zero_grad()\n",
        "                images, labels = sample['image'], sample['label']\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # 모델의 dropoupt, batchnormalization를 eval모드로 설정\n",
        "                model2.eval()\n",
        "                # .forward()에서 중간 노드의 gradient를 계산\n",
        "                with torch.no_grad():\n",
        "                    # acc, validation loss를 계산\n",
        "                    probs  = model2(images)\n",
        "                    valid_loss = criterion(probs, labels)\n",
        "\n",
        "                    # train accuracy 계산\n",
        "                    probs  = probs.cpu().detach().numpy()\n",
        "                    labels = labels.cpu().detach().numpy()\n",
        "                    preds = probs > 0.5\n",
        "                    batch_acc = (labels == preds).mean()\n",
        "                    valid_acc_list.append(batch_acc)\n",
        "                    valid_loss_list.append(valid_loss.item())\n",
        "                valid_acc = np.mean(valid_acc_list)\n",
        "                valid_loss = np.mean(valid_loss_list)\n",
        "                valid_bar.set_postfix(valid_loss = valid_loss,\n",
        "                                      valid_acc = valid_acc)\n",
        "            \n",
        "        # Learning rate 조절\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # 모델 저장\n",
        "        if valid_acc_max < valid_acc:\n",
        "            valid_acc_max = valid_acc\n",
        "            best_model = model2\n",
        "            MODEL = \"Effnetb7_3th\"\n",
        "            # 모델을 저장할 구글 드라이브 경로\n",
        "            path = \"/content/drive/MyDrive/Dacon_Emnist2/\"\n",
        "            torch.save(best_model, f'{path}{fold_index}_{MODEL}_{valid_loss.item():2.4f}_epoch_{epoch}.pth')\n",
        "\n",
        "    # 폴드별로 가장 좋은 모델 저장\n",
        "    best_models2.append(best_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtBZcaGSriXM"
      },
      "source": [
        "cd /content/."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1ze3UIHNmyC"
      },
      "source": [
        "# __Inference__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbcSLsHCaWZk"
      },
      "source": [
        "# 폴드별 best model 드라이브에서 로드하기\n",
        "# best_models = []\n",
        "# best_models.append(torch.load('./drive/MyDrive/Dacon_Emnist2/efficientnetb7_1_0.1780.pth'))\n",
        "# best_models.append(torch.load('./drive/MyDrive/Dacon_Emnist2/2_efficientnetb7_0.1672.pth'))\n",
        "# best_models.append(torch.load('./drive/MyDrive/Dacon_Emnist2/3_efficientnetb7_0.1581.pth'))\n",
        "# best_models.append(torch.load('./drive/MyDrive/Dacon_Emnist2/efficientnetb7_4_0.1610.pth'))\n",
        "# best_models.append(torch.load('./drive/MyDrive/Dacon_Emnist2/efficientnetb7_5_0.1645.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdYLBDQMU7Qa"
      },
      "source": [
        "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
        "test_dataset = DatasetMNIST(\"test_dirty_mnist/\", sample_submission)\n",
        "batch_size = 32\n",
        "test_data_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    num_workers = 3,\n",
        "    drop_last = False\n",
        ")\n",
        "test_data_loader2 = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = 128,\n",
        "    shuffle = False,\n",
        "    num_workers = 3,\n",
        "    drop_last = False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfTXnLQuU-wS"
      },
      "source": [
        "predictions_list = []\n",
        "# 배치 단위로 추론\n",
        "prediction_df = pd.read_csv(\"sample_submission.csv\")\n",
        "\n",
        "# 5개의 fold마다 가장 좋은 모델을 이용하여 예측\n",
        "for model in best_models:\n",
        "    # 0으로 채워진 array 생성\n",
        "    prediction_array = np.zeros([prediction_df.shape[0],\n",
        "                                 prediction_df.shape[1] -1])\n",
        "    for idx, sample in enumerate(test_data_loader):\n",
        "        with torch.no_grad():\n",
        "            # 추론\n",
        "            model.eval()\n",
        "            images = sample['image']\n",
        "            images = images.to(device)\n",
        "            probs  = model(images)\n",
        "            probs = probs.cpu().detach().numpy()\n",
        "            preds = (probs > 0.5)\n",
        "\n",
        "            # 예측 결과를 \n",
        "            # prediction_array에 입력\n",
        "            batch_index = batch_size * idx\n",
        "            prediction_array[batch_index: batch_index + images.shape[0],:]\\\n",
        "                         = preds.astype(int)\n",
        "                         \n",
        "    # 채널을 하나 추가하여 list에 append\n",
        "    predictions_list.append(prediction_array[...,np.newaxis])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOIFAVOcA-QC"
      },
      "source": [
        "predictions_list2 = []\n",
        "for model in best_models2:\n",
        "    # 0으로 채워진 array 생성\n",
        "    prediction_array2 = np.zeros([prediction_df.shape[0],\n",
        "                                 prediction_df.shape[1] -1])\n",
        "    for idx, sample in enumerate(test_data_loader2):\n",
        "        with torch.no_grad():\n",
        "            # 추론\n",
        "            model.eval()\n",
        "            images = sample['image']\n",
        "            images = images.to(device)\n",
        "            probs  = model(images)\n",
        "            probs = probs.cpu().detach().numpy()\n",
        "            preds = (probs > 0.5)                \n",
        "\n",
        "            # 예측 결과를 \n",
        "            # prediction_array에 입력\n",
        "            batch_index = batch_size * idx\n",
        "            prediction_array2[batch_index: batch_index + images.shape[0],:]\\\n",
        "                         = preds.astype(int)\n",
        "                         \n",
        "    # 채널을 하나 추가하여 list에 append\n",
        "    predictions_list2.append(prediction_array[...,np.newaxis])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXc-OFLvRpnR"
      },
      "source": [
        "# 모델 전체를 불러오는 것이 아니라 확률값을 불러와서 적용\n",
        "\n",
        "# prediction_array_song = np.load('./drive/MyDrive/Dacon_Emnist2/predictions_b7_3th_fold5_song.dat',allow_pickle=True)\n",
        "# predictions_array = np.concatenate(predictions_list, axis = 2)\n",
        "# predictions_array_final = np.concatenate([predictions_array, prediction_array_song], axis=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7tFBO99OHbQ"
      },
      "source": [
        "# __Ensemble__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV7EdFhfDT5p"
      },
      "source": [
        "########## Ensemble Code ##########\n",
        "predictions_array1 = np.concatenate(predictions_list, axis=2)\n",
        "predictions_array2 = np.concatenate(predictions_list2, axis=2)\n",
        "predictions_array_final = np.concatenate([predictions_array1, predictions_array2], axis=2)\n",
        "predictions_mean = predictions_array_final.mean(axis = 2)\n",
        "# 평균 값이 0.5보다 클 경우 1 작으면 0\n",
        "predictions_mean = (predictions_mean > 0.5) * 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7BqL1R0JQZO"
      },
      "source": [
        "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
        "sample_submission.iloc[:,1:] = predictions_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ88zxqmGi4_"
      },
      "source": [
        "cd /content/drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zSJSPXSVRJA"
      },
      "source": [
        "sample_submission.to_csv(\"./Dacon_Emnist2/final_test_prediction.csv\", index = False)\n",
        "sample_submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMigXfdhVTQ2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}